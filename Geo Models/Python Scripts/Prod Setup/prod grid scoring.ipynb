{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cx_Oracle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import date,datetime\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, cross_val_score \n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"D:\\HomeCredit\\Bigdata\\Common Maps KNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate grid based scores for selected variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query for Data Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop table bd_geo_delinq;\n",
    "# create table bd_geo_delinq as\n",
    "# select * from\n",
    "# (\n",
    "# select a.skp_credit_case, a.DATE_decision, a.latitude as cus_lat, a.longitude as cus_long, grid_id,\n",
    "# b.RISK_FPD10, b.RISK_FPD30, b.RISK_FSTQPD30, b.client_inr_income,\n",
    "# b.PRODUCT_GROUP,case when score_cb<300 then 'NTC' else 'Others' end  as NTC_FLAG,  \n",
    "# row_number() over (partition by a.skp_credit_case order by a.DATE_decision asc) as rnk\n",
    "# from AP_BI.bd_geo a \n",
    "# join  \n",
    "# (select * from\n",
    "# uw_daily_data_all \n",
    "# where time_decision_date >= trunc(sysdate)-500\n",
    "# )b\n",
    "# on a.skp_credit_case = b.skp_credit_case \n",
    "# where a.date_decision >= trunc(sysdate)-500\n",
    "# and a.latitude is not null \n",
    "# and a.longitude is not null \n",
    "# and a.result = 'FOUND'\n",
    "# and lower(DECISION_OVERALL) = 'approved'\n",
    "# ) a\n",
    "# where rnk = 1\n",
    "# ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_pickle(os.getcwd()+r\"\\Data Files\\prod_delinq_comb.pkl\") #Read the data from query/downloaded file\n",
    "df[\"DATE_DECISION\"] = pd.to_datetime(df.DATE_DECISION, format = '%d-%m-%y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of scores that need to be calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_list =['RISK_FPD10_SCORE','RISK_FPD30_SCORE','RISK_FSTQPD30_SCORE','RISK_FPD10_MM_SCORE',\n",
    "'DEFAULT_FLAG_CHANGE_1','NO_PAYMENT_CURRENT_PAYMENT_1','NO_PAYMENT_CURRENT_PAYMENT_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Hyperparameter file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_df= pd.read_csv(os.getcwd()+r\"\\Data Files\\Hyperparameters\\Hyperparameters.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Months for which Scores need to be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_month = pd.DataFrame({'month':[3],'year':[2020]}) #input the last month for which data is downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SKP_CREDIT_CASE</th>\n",
       "      <th>DATE_DECISION</th>\n",
       "      <th>CUS_LAT</th>\n",
       "      <th>CUS_LONG</th>\n",
       "      <th>GRID_ID</th>\n",
       "      <th>RISK_FPD10</th>\n",
       "      <th>RISK_FPD30</th>\n",
       "      <th>RISK_FSTQPD30</th>\n",
       "      <th>CLIENT_INR_INCOME</th>\n",
       "      <th>PRODUCT_GROUP</th>\n",
       "      <th>NTC_FLAG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>220372757</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>19.375193</td>\n",
       "      <td>72.825214</td>\n",
       "      <td>500_00897_02686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>CD</td>\n",
       "      <td>NTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>220373060</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>22.468687</td>\n",
       "      <td>73.285382</td>\n",
       "      <td>500_01024_03353</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>CD</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>220543584</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>25.429391</td>\n",
       "      <td>81.809539</td>\n",
       "      <td>500_02737_03966</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12799.4</td>\n",
       "      <td>CL</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>220543586</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>30.368656</td>\n",
       "      <td>76.846756</td>\n",
       "      <td>500_01781_05047</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12000.0</td>\n",
       "      <td>CD</td>\n",
       "      <td>NTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>220543587</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>17.707650</td>\n",
       "      <td>83.296908</td>\n",
       "      <td>500_03069_02293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>CD</td>\n",
       "      <td>Others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SKP_CREDIT_CASE DATE_DECISION    CUS_LAT   CUS_LONG          GRID_ID  \\\n",
       "0        220372757    2019-01-01  19.375193  72.825214  500_00897_02686   \n",
       "1        220373060    2019-01-01  22.468687  73.285382  500_01024_03353   \n",
       "2        220543584    2019-01-01  25.429391  81.809539  500_02737_03966   \n",
       "3        220543586    2019-01-01  30.368656  76.846756  500_01781_05047   \n",
       "4        220543587    2019-01-01  17.707650  83.296908  500_03069_02293   \n",
       "\n",
       "   RISK_FPD10  RISK_FPD30  RISK_FSTQPD30  CLIENT_INR_INCOME PRODUCT_GROUP  \\\n",
       "0         0.0         0.0            0.0            20000.0            CD   \n",
       "1         0.0         0.0            0.0            11000.0            CD   \n",
       "2         0.0         0.0            0.0            12799.4            CL   \n",
       "3         0.0         0.0            0.0            12000.0            CD   \n",
       "4         0.0         0.0            0.0            30000.0            CD   \n",
       "\n",
       "  NTC_FLAG  \n",
       "0      NTC  \n",
       "1   Others  \n",
       "2   Others  \n",
       "3      NTC  \n",
       "4   Others  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download all active Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select grid_id, avg(cus_lat), avg(cus_long) from bd_geo_delinquency\n",
    "# group by grid_id\n",
    "# ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_grids= pd.read_csv(os.getcwd()+r\"\\Data Files\\grid id lat long.csv\")\n",
    "all_grids.columns = [\"GRID_ID\",\"CUS_LAT\",\"CUS_LONG\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_test = pd.DataFrame()\n",
    "df_final_grid = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aman.khatri91425\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for predicting (month): 3  SCORE: RISK_FPD10_SCORE  target: RISK_FPD10  K: 271  observation_window: 365  performance_window: 40  downsample_size: 150000  GINI TRAIN: 15.55%  time taken:  2.7\n",
      "Training for predicting (month): 3  SCORE: RISK_FPD30_SCORE  target: RISK_FPD30  K: 136  observation_window: 365  performance_window: 60  downsample_size: 50000  GINI TRAIN: 20.67%  time taken:  0.8\n",
      "Training for predicting (month): 3  SCORE: RISK_FSTQPD30_SCORE  target: RISK_FSTQPD30  K: 376  observation_window: 365  performance_window: 150  downsample_size: 300000  GINI TRAIN: 14.62%  time taken:  6.5\n",
      "Training for predicting (month): 3  SCORE: RISK_FPD10_MM_SCORE  target: RISK_FPD10  K: 241  observation_window: 110  performance_window: 40  downsample_size: 50000  GINI TRAIN: 16.08%  time taken:  1.3\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "for t in df_month.index:\n",
    "    x=df_month[\"month\"][t]\n",
    "    year=df_month[\"year\"][t]\n",
    "    date_test_start = datetime.datetime(year, x, 1)\n",
    "    date_test_end = (date_test_start +  datetime.timedelta(days=31)).replace(day=1)\n",
    "    df_test = df[(df[\"DATE_DECISION\"]>=date_test_start) & \n",
    "                      (df[\"DATE_DECISION\"]<date_test_end)]\n",
    "    df_grid = all_grids\n",
    "    df_grid[\"MONTH\"] = date_test_start\n",
    "\n",
    "    for y in hyper_df.index:\n",
    "        if hyper_df.loc[y,'SCORE_NAME'] in selected_list:\n",
    "            score_name = hyper_df.loc[y,'SCORE_NAME']\n",
    "            target = hyper_df.loc[y,'TARGET']\n",
    "            K = hyper_df.loc[y,'K']\n",
    "            observation_window = hyper_df.loc[y,'OBSERVATION_WINDOW']\n",
    "            performance_window = hyper_df.loc[y,'PERFORMANCE_WINDOW']\n",
    "            downsample_size = hyper_df.loc[y,'DOWNSAMPLE_SIZE']\n",
    "            fillna = hyper_df.loc[y,'FILLNA']\n",
    "\n",
    "            date_train_start = (date_test_start-datetime.timedelta(days=int(performance_window))-datetime.timedelta(days=int(observation_window))) \n",
    "            date_train_end = (date_test_start-datetime.timedelta(days=int(performance_window)))\n",
    "            start_time = datetime.datetime.now()\n",
    "            df_train = df[(df[\"DATE_DECISION\"]>=date_train_start) & \n",
    "                      (df[\"DATE_DECISION\"]<date_train_end)]\n",
    "\n",
    "            df_class_0 = df_train[df_train[target] == 0]\n",
    "            df_class_1 = df_train[df_train[target] == 1]\n",
    "            df_class_0_under = df_class_0.sample(min(downsample_size,df_class_1.shape[0]))\n",
    "            df_class_1_under = df_class_1.sample(min(downsample_size,df_class_1.shape[0]))\n",
    "            df_train_under = pd.concat([df_class_0_under, df_class_1_under], axis=0)\n",
    "\n",
    "            X_train = df_train_under[[\"CUS_LAT\",\"CUS_LONG\"]]\n",
    "            X_test = df_test[[\"CUS_LAT\",\"CUS_LONG\"]]\n",
    "            y_train = df_train_under[target]\n",
    "            y_test =  df_test[target]\n",
    "\n",
    "            knn = KNeighborsClassifier(n_neighbors = K)\n",
    "            bst = knn.fit(X_train[[\"CUS_LAT\",\"CUS_LONG\"]], np.ravel(y_train))\n",
    "            \n",
    "            all_grids1 = all_grids\n",
    "            included_grids = pd.DataFrame(df_train_under[\"GRID_ID\"].unique())\n",
    "            included_grids.columns = ['GRID_ID']\n",
    "            all_grids1 = all_grids1.merge(included_grids, left_on = 'GRID_ID',right_on = 'GRID_ID', how = 'left',indicator = True)\n",
    "            missing_grids=all_grids1[all_grids1[\"_merge\"] != 'both'][[\"GRID_ID\",\"CUS_LAT\",\"CUS_LONG\"]]\n",
    "            df_train_under = pd.concat([df_train_under,missing_grids])\n",
    "\n",
    "            ypred_train = bst.predict_proba(df_train_under[[\"CUS_LAT\",\"CUS_LONG\"]])[:, 1]\n",
    "            ypred_train1 = bst.predict_proba(X_train[[\"CUS_LAT\",\"CUS_LONG\"]])[:, 1]  #Only for roc auc\n",
    "            \n",
    "            ypred_test = bst.predict_proba(X_test[[\"CUS_LAT\",\"CUS_LONG\"]])[:, 1]\n",
    "            metric_auc_train = metrics.roc_auc_score(y_train, ypred_train1)\n",
    "#            metric_auc_test = metrics.roc_auc_score(y_test, ypred_test)\n",
    "            gini_train = 2 * metric_auc_train - 1\n",
    "#            gini_test = 2 * metric_auc_test - 1\n",
    "\n",
    "            df_test[score_name] = ypred_test\n",
    "            \n",
    "            df_train_under [\"G_\"+score_name] = ypred_train\n",
    "            grid_score = df_train_under[[\"GRID_ID\",\"G_\"+score_name]].groupby(\"GRID_ID\").mean()[\"G_\"+score_name].reset_index()\n",
    "            grid_score.columns = ['GRID_ID',\"G_\"+score_name]\n",
    "            \n",
    "            df_test = pd.merge(df_test, grid_score, on = 'GRID_ID', how = 'left')\n",
    "            df_test[\"G_\"+score_name]=df_test[\"G_\"+score_name].fillna(value = fillna)\n",
    "            \n",
    "            df_grid =  pd.merge(df_grid, grid_score, on = 'GRID_ID', how = 'left')\n",
    "            \n",
    "            print(\"Training for predicting (month):\",date_test_start.month, \n",
    "                  \" SCORE:\",score_name,\n",
    "                  \" target:\",target,\n",
    "                  \" K:\",K,\n",
    "                  \" observation_window:\",observation_window,\n",
    "                  \" performance_window:\",performance_window,\n",
    "                  \" downsample_size:\",downsample_size,\n",
    "                  \" GINI TRAIN:\",'{:,.2%}'.format(gini_train), \n",
    "#                   \"    GINI TEST:\",'{:,.2%}'.format(gini_test),\n",
    "                  \" time taken: \",'{:,.2}'.format((datetime.datetime.now()-start_time).seconds/60))\n",
    "\n",
    "\n",
    "    if df_final_test.shape == (0,0):\n",
    "        df_final_test = df_test\n",
    "    else:\n",
    "        df_final_test = pd.concat([df_final_test,df_test],ignore_index=True)\n",
    "\n",
    "    if df_final_grid.shape == (0,0):\n",
    "        df_final_grid = df_grid\n",
    "    else:\n",
    "        df_final_grid = pd.concat([df_final_grid,df_grid],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_test.to_pickle(os.getcwd()+r\"\\Data Files\\prod_scores1.pkl\")\n",
    "df_final_grid.to_pickle(os.getcwd()+r\"\\Data Files\\prod_grid_scores1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop table bd_recent_delinquency;\n",
    "# create table bd_recent_delinquency as\n",
    "# select date_balance, cus_lat, cus_long, grid_id, DEFAULT_FLAG_CHANGE, NO_PAYMENT_CURRENT_PAYMENT\n",
    "# from\n",
    "# (\n",
    "# select date_balance, cus_lat, cus_long, grid_id, \n",
    "# DEFAULT_FLAG_CHANGE, NO_PAYMENT_CURRENT_PAYMENT,\n",
    "# row_number() over (partition by trunc(date_balance,'mon'), \n",
    "# DEFAULT_FLAG_CHANGE, NO_PAYMENT_CURRENT_PAYMENT order by DBMS_RANDOM.VALUE) as rnk\n",
    "# from\n",
    "# (\n",
    "# select \n",
    "# a.skp_credit_case, \n",
    "# a.skp_credit_type,\n",
    "# a.date_decision,\n",
    "# date_balance,\n",
    "# latitude cus_lat,\n",
    "# longitude cus_long, \n",
    "# grid_id, \n",
    "# dpd_current,\n",
    "# dpd_earlier, \n",
    "# case when dpd_current>0 and dpd_earlier <= 0 then 1 else 0 end as default_flag_change,\n",
    "# case when dpd_current - dpd_earlier > 27 then 1 else 0 end as no_payment_current_payment\n",
    "# from\n",
    "# (\n",
    "# select skp_credit_case, skp_credit_type,\n",
    "# date_decision,\n",
    "# date_balance,\n",
    "# cnt_days_past_due_tolerance dpd_current, \n",
    "# lag(cnt_days_past_due_tolerance,1) over (partition by skp_credit_case order by date_balance asc) dpd_earlier\n",
    "# from owner_dwh.f_account_balance_pos_cash_pm\n",
    "# where date_balance >= trunc(sysdate)-730\n",
    "# ) a \n",
    "# join \n",
    "# AP_BI.bd_geo b \n",
    "# on a.skp_credit_case = b.skp_credit_case\n",
    "# where latitude is not null\n",
    "# and longitude is not null\n",
    "# and grid_id is not null\n",
    "# and b.result = 'FOUND'\n",
    "# )\n",
    "# where date_balance >= trunc(sysdate)-730\n",
    "# )\n",
    "# where rnk <= 300000\n",
    "# ;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_pickle(os.getcwd()+r\"\\Data Files\\prod_recent_delinq_comb.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"DATE_BALANCE\"] = pd.to_datetime(df1.DATE_BALANCE, format = '%d-%m-%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_df= pd.read_csv(os.getcwd()+r\"\\Data Files\\Hyperparameters\\Hyperparameters_EMI_Payment_Trend.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_test = pd.DataFrame()\n",
    "df_final_grid = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aman.khatri91425\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for predicting (month): 3  SCORE: DEFAULT_FLAG_CHANGE_1  target: DEFAULT_FLAG_CHANGE  K: 136  observation_window: 30  performance_window: 0  downsample_size: 80000  GINI TRAIN: 20.77%  time taken:  0.92\n",
      "Training for predicting (month): 3  SCORE: NO_PAYMENT_CURRENT_PAYMENT_1  target: NO_PAYMENT_CURRENT_PAYMENT  K: 351  observation_window: 30  performance_window: 0  downsample_size: 300000  GINI TRAIN: 13.16%  time taken:  3.4\n",
      "Training for predicting (month): 3  SCORE: NO_PAYMENT_CURRENT_PAYMENT_2  target: NO_PAYMENT_CURRENT_PAYMENT  K: 351  observation_window: 30  performance_window: 30  downsample_size: 300000  GINI TRAIN: 11.79%  time taken:  3.4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import datetime\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "for t in df_month.index:\n",
    "    x=df_month[\"month\"][t]\n",
    "    year=df_month[\"year\"][t]\n",
    "    date_test_start = datetime.datetime(year, x, 1)\n",
    "    date_test_end = (date_test_start +  datetime.timedelta(days=31)).replace(day=1)\n",
    "    df_test = df[(df[\"DATE_DECISION\"]>=date_test_start) & \n",
    "                      (df[\"DATE_DECISION\"]<date_test_end)]\n",
    "    df_grid = all_grids\n",
    "    df_grid[\"MONTH\"] = date_test_start\n",
    "\n",
    "    for y in hyper_df.index:\n",
    "#         if hyper_df.loc[y,'MONTH_HIST'] < x or year == 2020:\n",
    "            if hyper_df.loc[y,'SCORE_NAME'] in selected_list:\n",
    "                score_name = hyper_df.loc[y,'SCORE_NAME']\n",
    "                target = hyper_df.loc[y,'TARGET']\n",
    "                K = hyper_df.loc[y,'K']\n",
    "                observation_window = hyper_df.loc[y,'OBSERVATION_WINDOW']\n",
    "                performance_window = hyper_df.loc[y,'PERFORMANCE_WINDOW']\n",
    "                downsample_size = hyper_df.loc[y,'DOWNSAMPLE_SIZE']\n",
    "\n",
    "                date_train_start = (date_test_start-datetime.timedelta(days=int(performance_window)-1)-datetime.timedelta(days=int(observation_window))).replace(day=1) \n",
    "                date_train_end = (date_test_start-datetime.timedelta(days=int(performance_window)-2)).replace(day=1)\n",
    "                start_time = datetime.datetime.now()\n",
    "                df_train = df1[(df1[\"DATE_BALANCE\"]>=date_train_start) & \n",
    "                          (df1[\"DATE_BALANCE\"]<date_train_end)]\n",
    "\n",
    "                df_class_0 = df_train[df_train[target] == 0]\n",
    "                df_class_1 = df_train[df_train[target] == 1]\n",
    "                df_class_0_under = df_class_0.sample(min(downsample_size,df_class_1.shape[0]))\n",
    "                df_class_1_under = df_class_1.sample(min(downsample_size,df_class_1.shape[0]))\n",
    "                df_train_under = pd.concat([df_class_0_under, df_class_1_under], axis=0)\n",
    "\n",
    "                X_train = df_train_under[[\"CUS_LAT\",\"CUS_LONG\"]]\n",
    "                X_test = df_test[[\"CUS_LAT\",\"CUS_LONG\"]]\n",
    "                y_train = df_train_under[target]\n",
    "                y_test =  df_test['RISK_FSTQPD30']\n",
    "\n",
    "                knn = KNeighborsClassifier(n_neighbors = K)\n",
    "                bst = knn.fit(X_train[[\"CUS_LAT\",\"CUS_LONG\"]], np.ravel(y_train))\n",
    "\n",
    "                \n",
    "                \n",
    "                all_grids1 = all_grids\n",
    "                included_grids = pd.DataFrame(df_train_under[\"GRID_ID\"].unique())\n",
    "                included_grids.columns = ['GRID_ID']\n",
    "                all_grids1 = all_grids1.merge(included_grids, left_on = 'GRID_ID',right_on = 'GRID_ID', how = 'left',indicator = True)\n",
    "                missing_grids=all_grids1[all_grids1[\"_merge\"] != 'both'][[\"GRID_ID\",\"CUS_LAT\",\"CUS_LONG\"]]\n",
    "                df_train_under = pd.concat([df_train_under,missing_grids])\n",
    "\n",
    "                ypred_train = bst.predict_proba(df_train_under[[\"CUS_LAT\",\"CUS_LONG\"]])[:, 1]\n",
    "                ypred_train1 = bst.predict_proba(X_train[[\"CUS_LAT\",\"CUS_LONG\"]])[:, 1]  #Only for roc auc\n",
    "\n",
    "                ypred_test = bst.predict_proba(X_test[[\"CUS_LAT\",\"CUS_LONG\"]])[:, 1]\n",
    "                metric_auc_train = metrics.roc_auc_score(y_train, ypred_train1)\n",
    "\n",
    "#                 metric_auc_test = metrics.roc_auc_score(y_test, ypred_test)\n",
    "                gini_train = 2 * metric_auc_train - 1\n",
    "#                 gini_test = 2 * metric_auc_test - 1\n",
    "\n",
    "                df_test[score_name] = ypred_test\n",
    "                df_train_under [\"G_\"+score_name] = ypred_train\n",
    "                grid_score = df_train_under[[\"GRID_ID\",\"G_\"+score_name]].groupby(\"GRID_ID\").mean()[\"G_\"+score_name].reset_index()\n",
    "                grid_score.columns = ['GRID_ID',\"G_\"+score_name]\n",
    "\n",
    "                df_test = pd.merge(df_test, grid_score, on = 'GRID_ID', how = 'left')\n",
    "                df_test[\"G_\"+score_name]=df_test[\"G_\"+score_name].fillna(value = fillna)\n",
    "                df_grid =  pd.merge(df_grid, grid_score, on = 'GRID_ID', how = 'left')\n",
    "\n",
    "\n",
    "                print(\"Training for predicting (month):\",date_test_start.month, \n",
    "                      \" SCORE:\",score_name,\n",
    "                      \" target:\",target,\n",
    "                      \" K:\",K,\n",
    "                      \" observation_window:\",observation_window,\n",
    "                      \" performance_window:\",performance_window,\n",
    "                      \" downsample_size:\",downsample_size,\n",
    "                      \" GINI TRAIN:\",'{:,.2%}'.format(gini_train),\n",
    "#                       \"    GINI TEST:\",'{:,.2%}'.format(gini_test),\n",
    "                      \" time taken: \",'{:,.2}'.format((datetime.datetime.now()-start_time).seconds/60))\n",
    "#         else:\n",
    "#             print(\"History not available\")\n",
    "\n",
    "\n",
    "    if df_final_test.shape == (0,0):\n",
    "        df_final_test = df_test\n",
    "    else:\n",
    "        df_final_test = pd.concat([df_final_test,df_test],ignore_index=True)\n",
    "        \n",
    "    \n",
    "    if df_final_grid.shape == (0,0):\n",
    "        df_final_grid = df_grid\n",
    "    else:\n",
    "        df_final_grid = pd.concat([df_final_grid,df_grid],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_test.to_pickle(os.getcwd()+r\"\\Data Files\\prod_scores2.pkl\")\n",
    "df_final_grid.to_pickle(os.getcwd()+r\"\\Data Files\\prod_grid_scores2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grid=pd.read_pickle(os.getcwd()+r\"\\Data Files\\prod_grid_scores1.pkl\")\n",
    "df1_grid= pd.read_pickle(os.getcwd()+r\"\\Data Files\\prod_grid_scores2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2530785, 11)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_grid = df1_grid[['GRID_ID','MONTH','G_DEFAULT_FLAG_CHANGE_1',\n",
    "          'G_NO_PAYMENT_CURRENT_PAYMENT_1','G_NO_PAYMENT_CURRENT_PAYMENT_2']]\n",
    "df_grid= df_grid.merge(df1_grid, on = ['GRID_ID','MONTH'],how = 'right')\n",
    "df_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_columns= [ 'G_RISK_FPD10_SCORE','G_RISK_FPD30_SCORE','G_RISK_FSTQPD30_SCORE','G_RISK_FPD10_MM_SCORE','G_DEFAULT_FLAG_CHANGE_1',\n",
    "                 'G_NO_PAYMENT_CURRENT_PAYMENT_1','G_NO_PAYMENT_CURRENT_PAYMENT_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert All Scores to Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in scores_columns:\n",
    "    df_grid[\"LOGIT_\"+y] = [np.log(x/(1-x)) for x in df_grid[y]]\n",
    "#     df[\"LOGIT_\"+y] = np.log(df[y]/(1-df[y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_columns= [x for x in df_grid.columns if 'LOGIT' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"COMBINED_SCORE_KNN_EXI\"] = 1/(1+np.exp(-(df1[\"LOGIT_G_RISK_FPD30_SCORE\"]*0.133694+ \n",
    "              df1[\"LOGIT_G_RISK_FSTQPD30_SCORE\"]*0.253384 +\n",
    "              df1[\"LOGIT_G_DEFAULT_FLAG_CHANGE_1\"]*0.138207 +\n",
    "              df1[\"LOGIT_G_NO_PAYMENT_CURRENT_PAYMENT_2\"]*0.298317 +\n",
    "              df1[\"LOGIT_G_RISK_FPD10_MM_SCORE\"]*0.189856 +\n",
    "              df1[\"LOGIT_G_RISK_FPD10_SCORE\"]*0.163698 +  -2.7974444)))\n",
    "df1[\"COMBINED_SCORE_KNN_NTC\"] =1/(1+np.exp(-(df1[\"LOGIT_G_RISK_FSTQPD30_SCORE\"]*0.495134+ \n",
    "              df1[\"LOGIT_G_NO_PAYMENT_CURRENT_PAYMENT_1\"]*0.246826 +\n",
    "                df1[\"LOGIT_G_DEFAULT_FLAG_CHANGE_1\"]*0.128352 +\n",
    "              df1[\"LOGIT_G_RISK_FPD10_MM_SCORE\"]*0.123364 +  -2.37891549)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(os.getcwd()+r\"\\Data Files\\prod_grid_scored.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data for Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[[\"GRID_ID\",\"MONTH\",\"COMBINED_SCORE_KNN_EXI\",\"COMBINED_SCORE_KNN_NTC\"]].to_csv(os.getcwd()+r\"\\Data Files\\prod_grid_scored_upload.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "upload the data in the following table:\n",
    "ap_uwi.bd_grid_score_hist\n",
    "\n",
    "\n",
    "Run the below code to update the current tables. Cordinate with Aditya to refresh the PE data post updation of below 2 tables.\n",
    "truncate table bd_geo_exi;\n",
    "insert into bd_geo_EXI as \n",
    "select a.grid_id, nvl(COMBINED_SCORE_KNN_EXI,0.05909955379812824) COMBINED_SCORE_KNN_EXI from AP_BI.T_SDE_GRID_500MMU a\n",
    "left join (select * from \n",
    "bd_grid_score_hist\n",
    "where month = (select max(month) from \n",
    "bd_grid_score_hist\n",
    ")) b\n",
    "on a.grid_id = b.grid_id\n",
    ";\n",
    "\n",
    "\n",
    "\n",
    "truncate table bd_geo_ntc;\n",
    "into into bd_geo_NTC as \n",
    "select a.grid_id, nvl(COMBINED_SCORE_KNN_NTC,0.08894919166065231) COMBINED_SCORE_KNN_NTC from AP_BI.T_SDE_GRID_500MMU a\n",
    "left join (select * from \n",
    "bd_grid_score_hist\n",
    "where month = (select max(month) from \n",
    "bd_grid_score_hist\n",
    ")) b\n",
    "on a.grid_id = b.grid_id\n",
    ";\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
